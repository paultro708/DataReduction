{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.datasets import fetch_rcv1\n",
    "from sklearn import datasets\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "import time\n",
    "from scipy.spatial import distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "data_all, data_label= datasets.load_iris(return_X_y=True)\n",
    "data_all_train, data_all_test, data_label_train, data_label_test = train_test_split(data_all, data_label, test_size=0.3)\n",
    "\n",
    "number_of_classes = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#temporary\n",
    "colors = np.array(['red', 'blue', 'green', 'orange', 'darkgray', 'powderblue', 'lightsalmon', 'cyan', 'pink'])\n",
    "#plt.scatter(x=datanpy[:,0], y=datanpy[:,1], c=colors[clustering.labels_])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(data_all_train[:,0], data_all_train[:,2],c=data_label_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(data_all_test[:,0], data_all_test[:,2],c=data_label_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(data_all[:,0], data_all[:,2],c=data_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.DataFrame(data_all_train)\n",
    "df_label = pd.DataFrame(data_label_train)\n",
    "df_label = df_label.rename(columns={0:\"label\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "n_features = data_all[0].shape[0]\n",
    "dr_train_with_label = pd.concat([df_train, df_label], axis=1)\n",
    "grouped = dr_train_with_label.groupby('label')\n",
    "'''\n",
    "#create list grouped indexes of each class\n",
    "class_id_gr=[]\n",
    "class_id_gr = defaultdict(list)\n",
    "for idx, label in enumerate(data_label_train):\n",
    "    class_id_gr[label].append(idx)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "[[5.4, 3.9, 1.7, 0.4],\n [4.9, 3.0, 1.4, 0.2],\n [5.2, 3.4, 1.4, 0.2],\n [4.8, 3.0, 1.4, 0.1],\n [5.4, 3.4, 1.7, 0.2],\n [5.0, 3.0, 1.6, 0.2],\n [4.8, 3.4, 1.6, 0.2],\n [4.8, 3.4, 1.9, 0.2],\n [5.1, 3.8, 1.5, 0.3],\n [5.8, 4.0, 1.2, 0.2],\n [5.4, 3.7, 1.5, 0.2],\n [5.0, 3.3, 1.4, 0.2],\n [5.1, 3.5, 1.4, 0.3],\n [5.2, 4.1, 1.5, 0.1],\n [5.0, 3.6, 1.4, 0.2],\n [5.1, 3.8, 1.9, 0.4],\n [5.1, 3.8, 1.6, 0.2],\n [4.6, 3.6, 1.0, 0.2],\n [4.7, 3.2, 1.6, 0.2],\n [4.8, 3.0, 1.4, 0.3],\n [5.0, 3.2, 1.2, 0.2],\n [5.7, 3.8, 1.7, 0.3],\n [4.4, 3.0, 1.3, 0.2],\n [5.4, 3.9, 1.3, 0.4],\n [5.1, 3.7, 1.5, 0.4],\n [5.1, 3.4, 1.5, 0.2],\n [5.5, 3.5, 1.3, 0.2],\n [4.8, 3.1, 1.6, 0.2],\n [4.6, 3.4, 1.4, 0.3],\n [5.0, 3.5, 1.6, 0.6],\n [5.1, 3.5, 1.4, 0.2]]"
     },
     "metadata": {},
     "execution_count": 113
    }
   ],
   "source": [
    " class_data_gr[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "defaultdict(<class 'list'>, {1: [0, 2, 3, 4, 5, 8, 10, 12, 14, 16, 17, 21, 24, 25, 26, 27, 34, 38, 40, 41, 43, 52, 58, 59, 62, 65, 66, 68, 73, 74, 81, 85, 88, 90, 91, 94, 95, 97, 101, 104], 2: [1, 6, 7, 9, 15, 22, 23, 30, 32, 39, 42, 44, 45, 47, 48, 49, 53, 55, 56, 60, 61, 63, 76, 78, 80, 82, 83, 84, 89, 92, 98, 99, 100, 103], 0: [11, 13, 18, 19, 20, 28, 29, 31, 33, 35, 36, 37, 46, 50, 51, 54, 57, 64, 67, 69, 70, 71, 72, 75, 77, 79, 86, 87, 93, 96, 102]})\n[8 4 5 4 2 4 0 0 1 6 2 5 5 1 5 1 1 7 0 4 4 8 3 6 1 5 2 4 0 0 5]\n"
    }
   ],
   "source": [
    "'''\n",
    "TODO dla każdej klasy wygenerować n klastrów\n",
    "najpierw zbiór pogrupować\n",
    "'''\n",
    "#creating clusters kmeans\n",
    "\n",
    "\n",
    "# class_data_gr = []\n",
    "\n",
    "# for i in range(number_of_classes):\n",
    "#     for el in range(len(class_id_gr[i])):\n",
    "#         class_data_gr[i].append(data_all_train[el])\n",
    "  \n",
    "# for item, key in class_id_gr:\n",
    "#     class_data_gr[key].append(data_all_train[item])\n",
    "\n",
    "#initialize\n",
    "class_data_gr = [] #np.array([])\n",
    "for i in range(number_of_classes):\n",
    "     class_data_gr.append([])\n",
    "#create array with data for each id in class\n",
    "for i in range(number_of_classes):\n",
    "    for instance_id in class_id_gr[i]:\n",
    "        class_data_gr[i].append(data_all_train[instance_id].tolist())\n",
    "\n",
    "# #prepare array for clustering\n",
    "# class_data_gr2 = [] #np.array([])\n",
    "# for i in range(number_of_classes):\n",
    "#      class_data_gr2.append([])\n",
    "\n",
    "# for i in range(number_of_classes):\n",
    "#     for instance_id in class_data_gr[i]:\n",
    "#         class_data_gr2[i].append(instance_id.tolist())\n",
    "#######\n",
    "\n",
    "print(class_id_gr)\n",
    "\n",
    "for i in range(number_of_classes)\n",
    "\n",
    "number_of_clusters = 3* number_of_classes\n",
    "clust= KMeans(n_clusters=number_of_clusters)\n",
    "clust.fit(class_data_gr[0])\n",
    "#print label of clusters\n",
    "print(clust.labels_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "[5 0 2 0 8 0 3 3 7 1 8 2 2 1 2 7 7 4 3 0 0 5 6 1 7 2 8 0 3 2 2]\n[1 2 2 1 8 8 6 4 8 0 5 8 1 8 0 2 6 3 7 1 7 5 0 6 4 4 6 8 4 7 1 7 6 0 1 3 6\n 5 7 4]\n[3 8 4 1 4 0 1 5 6 2 0 2 3 0 2 1 1 8 2 0 5 1 2 6 1 5 5 8 7 6 3 8 6 3]\n"
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "[[defaultdict(list,\n              {5: [11, 71],\n               0: [13, 19, 28, 69, 70, 87],\n               2: [18, 37, 46, 51, 79, 96, 102],\n               8: [20, 36, 86],\n               3: [29, 31, 67, 93],\n               7: [33, 54, 57, 77],\n               1: [35, 50, 75],\n               4: [64],\n               6: [72]})],\n [defaultdict(list,\n              {1: [0, 4, 24, 41, 81, 91],\n               2: [2, 3, 27],\n               8: [5, 8, 14, 21, 25, 68],\n               6: [10, 34, 59, 66, 88, 95],\n               4: [12, 62, 65, 73, 104],\n               0: [16, 26, 58, 90],\n               5: [17, 52, 97],\n               3: [38, 94],\n               7: [40, 43, 74, 85, 101]})],\n [defaultdict(list,\n              {3: [1, 45, 98, 103],\n               8: [6, 55, 84, 99],\n               4: [7, 15],\n               1: [9, 23, 49, 53, 63, 80],\n               0: [22, 42, 47, 60],\n               5: [30, 61, 82, 83],\n               6: [32, 78, 92, 100],\n               2: [39, 44, 48, 56, 76],\n               7: [89]})]]"
     },
     "metadata": {},
     "execution_count": 121
    }
   ],
   "source": [
    "#initialize\n",
    "clusters = [] #np.array([])\n",
    "for i in range(number_of_classes):\n",
    "     clusters.append([])\n",
    "\n",
    "#create clusters for each class\n",
    "for i in range(number_of_classes):\n",
    "    number_of_clusters = 3* number_of_classes\n",
    "    clust = KMeans(n_clusters=number_of_clusters)\n",
    "    clust.fit(class_data_gr[i])\n",
    "    #print label of clusters\n",
    "    print(clust.labels_)\n",
    "\n",
    "    clusters_with_id = defaultdict(list)\n",
    "    for idx, cluster in enumerate(clust.labels_):\n",
    "        clusters_with_id[cluster].append(class_id_gr[i][idx])\n",
    "\n",
    "    clusters[i].append(clusters_with_id)\n",
    "\n",
    "clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "defaultdict(list,\n            {8: [0, 21],\n             4: [1, 3, 5, 19, 27],\n             1: [2, 11, 12, 14, 20, 25, 30],\n             7: [4, 10, 26],\n             6: [6, 7, 18, 28],\n             0: [8, 15, 16, 24, 29],\n             2: [9, 13, 23],\n             5: [17],\n             3: [22]})"
     },
     "metadata": {},
     "execution_count": 117
    }
   ],
   "source": [
    "''''''''''''''''''''''''from collections import defaultdict\n",
    "\n",
    "#creating list grouping idexes of training data grouped by cluster label\n",
    "clusters_with_id = defaultdict(list)\n",
    "for idx, cluster in enumerate(clust.labels_):\n",
    "    clusters_with_id[cluster].append(idx)\n",
    "\n",
    "clusters_with_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def median_in_cluster(data_all, indexes):\n",
    "    count_of_values = len(indexes)\n",
    "    sum = 0\n",
    "\n",
    "    # liczba wszystkich wymiarów wzgledem których ma być sumowane\n",
    "    count_of_features = data_all[0].shape[0]\n",
    "\n",
    "    median = np.array([])\n",
    "    \n",
    "    for feature in range(count_of_features):\n",
    "        for index in indexes:\n",
    "            actual_data = data_all[index]\n",
    "            sum += actual_data[feature]\n",
    "        median = np.append(median, sum/count_of_values)\n",
    "        sum = 0\n",
    "\n",
    "    return median"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Function finding id of point in data, nearest to given point. Using for find nerest point of mean in homogeniuos cluster \n",
    "TODO czy ma zwracać kilka indesów jeżeli takie same odległosci?\n",
    "\"\"\"\n",
    "def find_id_of_nearest_point(data_all, indexes, point):\n",
    "    #id of nearest, for now the first\n",
    "    id = indexes[0] \n",
    "    #minimal distance, for now - the first distance\n",
    "    min_dist = distance.euclidean(point, data_all[id])\n",
    "\n",
    "    for i in indexes:\n",
    "        data = data_all[i]\n",
    "        dist = distance.euclidean(point, data)\n",
    "        if min_dist > dist:\n",
    "            min_dist = dist\n",
    "            id = i\n",
    "\n",
    "    return id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Function return index of nearest instance to given\n",
    "element - index of instance \n",
    "indexes_of_data - indexes of instances from which have to find nearest to element\n",
    "data_all - array of all instances to get data of selected index\n",
    "\"\"\"\n",
    "def  find_nearest_instance(element, indexes_of_data, data_all):\n",
    "    point = data_all[element]\n",
    "    #first temporary index\n",
    "    id = 0\n",
    "    #minimal distance, for now - the first distance\n",
    "    min_dist = distance.euclidean(point, data_all[id])\n",
    "    for i in indexes_of_data:\n",
    "        if i == element:\n",
    "            break\n",
    "        data = data_all[i]\n",
    "        dist = distance.euclidean(point, data)\n",
    "        if min_dist > dist:\n",
    "            min_dist = dist\n",
    "            id = i\n",
    "\n",
    "    return id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "data_all - data from getting exact\n",
    "indexes - indexes of data from data_all\n",
    "'''\n",
    "def data_for_indexes(data_all, indexes):\n",
    "    data_indexes = np.array([])\n",
    "    for id in indexes:\n",
    "        data_indexes = np.append(data_indexes, data_all[id])\n",
    "\n",
    "    return data_indexes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Function return index of majority class\n",
    "\"\"\"\n",
    "def  find_majority_class(number_of_classes, classes_with_indexes):\n",
    "    max = len(classes_with_indexes[0])\n",
    "    majority_class = 0\n",
    "\n",
    "    for i in range(number_of_classes):\n",
    "        count = len(classes_with_indexes[i])\n",
    "        if max < count:\n",
    "            max = count\n",
    "            majority_class = i\n",
    "\n",
    "    return majority_class\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_point_in_cluster(data_all, indexes):\n",
    "    count_of_values = len(indexes)\n",
    "    sum = 0\n",
    "\n",
    "    # liczba wszystkich wymiarów wzgledem których ma być sumowane\n",
    "    count_of_features = data_all[0].shape[0]\n",
    "\n",
    "    mean_point = np.array([])\n",
    "    \n",
    "    for feature in range(count_of_features):\n",
    "        sum = 0\n",
    "        for index in indexes:\n",
    "            actual_data = data_all[index]\n",
    "            sum += actual_data[feature]\n",
    "        mean_point = np.append(mean_point, sum/count_of_values)\n",
    "\n",
    "    return mean_point\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "classes count: [0. 6. 3.]\nclasses data with id: [[], [5, 0, 2, 8, 3, 4], [7, 1, 6]]\n1\n"
    },
    {
     "output_type": "error",
     "ename": "IndexError",
     "evalue": "list index out of range",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-122-c8a5662636d9>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     98\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     99\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnumber_of_classes\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 100\u001b[1;33m     \u001b[0mreduced_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreduced_set\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mclustering_reduction\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mclusters\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    101\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-122-c8a5662636d9>\u001b[0m in \u001b[0;36mclustering_reduction\u001b[1;34m(clusters_with_id)\u001b[0m\n\u001b[0;32m     29\u001b[0m     \u001b[1;31m#zliczamy liczbę wystąpień każdej z klas\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     30\u001b[0m     \u001b[1;31m#dla każdego id wystepującej danej w klastrze sprawdzić klasę i zwiększyć liczbę wystąpień\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 31\u001b[1;33m         \u001b[1;32mfor\u001b[0m \u001b[0minstance_id\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mclusters_with_id\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     32\u001b[0m             \u001b[0mclass_label_of_instance\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata_label_train\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0minstance_id\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     33\u001b[0m         \u001b[1;31m#to teraz dodajemy do listy danych instancji tej klasy\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "#clusters_with_id[key-czyli label of cluster :)]\n",
    "#clusters_with_id[6]\n",
    "\n",
    "\n",
    "\n",
    "'''\n",
    "Function do the clustering method of reduction\n",
    "'''\n",
    "def clustering_reduction(clusters_with_id):\n",
    "        #utowrzenie tablicy n wartości, gdzie n to liczba klas, w której bedzie trzymana liczba wystapień danej klasy\n",
    "    #initialize array with 0 occurrence of each class\n",
    "    classes_with_data = [] #zestaw instancji danych dla każdej z klasy tj np classes_data[0] - dane instancji klasy pierwszej w klastrze\n",
    "    classes_with_indexes = []\n",
    "    #create empty reduced dataset\n",
    "    reduced_set = []\n",
    "    reduced_data = []\n",
    "    \n",
    "    classes_count = np.array([])\n",
    "    for i in range(number_of_classes):\n",
    "        classes_count = np.append(classes_count, 0)\n",
    "        classes_with_data.append([])\n",
    "        classes_with_indexes.append([])\n",
    "        reduced_set.append([])\n",
    "\n",
    "\n",
    "#dla każdego klastra\n",
    "#for each cluster\n",
    "    for i in range(number_of_clusters):\n",
    "    #zliczamy liczbę wystąpień każdej z klas\n",
    "    #dla każdego id wystepującej danej w klastrze sprawdzić klasę i zwiększyć liczbę wystąpień\n",
    "        for instance_id in clusters_with_id[i]:\n",
    "            class_label_of_instance = data_label_train[instance_id]\n",
    "        #to teraz dodajemy do listy danych instancji tej klasy \n",
    "        #classes_data.append(data_all_train[instance_id])\n",
    "            classes_with_data[class_label_of_instance].append(data_all_train[instance_id]) #dodajemy do\n",
    "        #classes_with_data= np.append(classes_with_data[class_label_of_instance], data_all_train(instance_id))\n",
    "            classes_with_indexes[class_label_of_instance].append(instance_id)\n",
    "            classes_count[class_label_of_instance]+=1\n",
    "        print('classes count:',classes_count)\n",
    "        print('classes data with id:',classes_with_indexes)\n",
    "\n",
    "        is_homogeniuos = True\n",
    "        count_of_classes_in_cluster = 0\n",
    "        for i in range(number_of_classes):\n",
    "            if(classes_count[i] > 0):\n",
    "                count_of_classes_in_cluster+=1\n",
    "        if (count_of_classes_in_cluster > 1):\n",
    "            is_homogeniuos = False\n",
    "\n",
    "\n",
    "        if (is_homogeniuos):\n",
    "            print('homogenious')\n",
    "            cm = find_majority_class(number_of_classes, classes_with_indexes)\n",
    "        #obliczyć średnia wszystkich wartości instancji w klastrze w postaci obiektu-\n",
    "        #dla każdej instancji obliczyć odległość euklidesową od tej średniej i zapisać w jakiejś pomocniczej np.array?\n",
    "        #wybrać \n",
    "            mean_point = mean_point_in_cluster(data_all=data_all_train, indexes=clusters_with_id[i])\n",
    "        #print(mean_point)\n",
    "            accept_id = find_id_of_nearest_point(data_all = data_all_train, indexes = clusters_with_id[i], point = mean_point)\n",
    "        #print(accept_id)\n",
    "            reduced_set[cm].append(data_all_train[accept_id])\n",
    "            reduced_data.append(data_all_train[accept_id])\n",
    "        \n",
    "        else:\n",
    "        #majority class\n",
    "            cm = find_majority_class(number_of_classes, classes_with_indexes)\n",
    "            print(cm)\n",
    "\n",
    "            for class_id in range(number_of_classes):\n",
    "                if class_id == cm:\n",
    "                    break\n",
    "                for el in classes_with_indexes[class_id]:\n",
    "                    #najbliższy element do badanego spośród głownej klasy\n",
    "                    nearest_of_majority_class = find_nearest_instance(element = el, indexes_of_data = classes_with_indexes[cm], data_all = data_all_train)\n",
    "                    print('nearest_of_majority_class:',nearest_of_majority_class)\n",
    "                    #reduced_set = np.append(reduced_set, data_all_train[nearest_of_majority_class])\n",
    "                    reduced_set[cm].append(data_all_train[nearest_of_majority_class])\n",
    "                    reduced_data.append(data_all_train[nearest_of_majority_class])\n",
    "                    #najbliżsy element do badanego spośród tej samej klasy co badany\n",
    "                    nearest_of_actual_class = find_nearest_instance(element = el, indexes_of_data = classes_with_indexes[class_id], data_all = data_all_train)\n",
    "                    print('nearest_of_actual_class:', nearest_of_actual_class)\n",
    "                    #reduced_set = np.append(reduced_set, data_all_train[nearest_of_actual_class])\n",
    "                    reduced_set[cm].append(data_all_train[nearest_of_actual_class])\n",
    "                    reduced_data.append(data_all_train[nearest_of_actual_class])\n",
    "\n",
    "\n",
    "            \n",
    "        #reset classes counter     \n",
    "        classes_count = np.array([])\n",
    "        classes_with_data = [] #zestaw instancji danych dla każdej z klasy tj np classes_data[0] - dane instancji klasy pierwszej w klastrze\n",
    "        classes_with_indexes = []\n",
    "        for i in range(number_of_classes):\n",
    "            classes_count = np.append(classes_count, 0)\n",
    "            classes_with_data.append([])\n",
    "            classes_with_indexes.append([])\n",
    "\n",
    "    return reduced_data, reduced_set\n",
    "\n",
    "for i in range(number_of_classes):\n",
    "    reduced_data, reduced_set = clustering_reduction(clusters[i])\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reduced_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#final labels\n",
    "reduced_colors = []\n",
    "\n",
    "for i in range(number_of_classes):\n",
    "    for id in reduced_set[i]:\n",
    "        reduced_colors.append(i)\n",
    "\n",
    "\n",
    "colors = {0:'red',1:'green',2:'blue', 3: 'purple'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test\n",
    "t = time.process_time()\n",
    "#knn classify - measure time and accuracy for original train dataset\n",
    "knn = KNeighborsClassifier(n_neighbors=5)\n",
    "knn.fit(data_all_train,data_label_train)\n",
    "accuracy = knn.score(data_all_test,data_label_test)\n",
    "elapsed = time.process_time() - t\n",
    "\n",
    "print(\"Time:  \", elapsed)\n",
    "print('Accuracy:  ', accuracy)\n",
    "print('Count of instances', len(data_all_train))\n",
    "\n",
    "t = time.process_time()\n",
    "knn2 = KNeighborsClassifier(n_neighbors=5)\n",
    "knn2.fit(np.array(reduced_data),np.array(reduced_colors))\n",
    "accuracy = knn2.score(data_all_test,data_label_test)\n",
    "elapsed = time.process_time() - t\n",
    "\n",
    "print(\"Time:  \", elapsed)\n",
    "print('Accuracy:  ', accuracy)\n",
    "print('Count of instances', len(reduced_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(data_all_train[:,0], data_all_train[:,2],c=data_label_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_all_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np_red_data = np.array(reduced_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_label_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np_red_col = np.array(reduced_colors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(np_red_data[:,0], np_red_data[:,2],c=np_red_col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": 3
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python_defaultSpec_1602483376976",
   "display_name": "Python 3.7.7 64-bit"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
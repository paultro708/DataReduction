# -*- coding: utf-8 -*-
"""clustering2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1MGvIyDk1l4Xh2oxuZ3D--zCNhyProqVN
"""

import numpy as np
import matplotlib.pyplot as plt
import pandas as pd
from sklearn.cluster import KMeans
from sklearn.datasets import fetch_rcv1
from sklearn import datasets
from sklearn.neighbors import KNeighborsClassifier
import time
from scipy.spatial import distance

#iris
from sklearn.model_selection import train_test_split
data_all, data_label= datasets.load_iris(return_X_y=True)
data_all_train, data_all_test, data_label_train, data_label_test = train_test_split(data_all, data_label, test_size=0.3)

number_of_classes = 3

#temporary
colors = np.array(['red', 'blue', 'green', 'orange', 'darkgray', 'powderblue', 'lightsalmon', 'cyan', 'pink'])
#plt.scatter(x=datanpy[:,0], y=datanpy[:,1], c=colors[clustering.labels_])

plt.scatter(data_all_train[:,0], data_all_train[:,2],c=data_label_train)

plt.scatter(data_all_test[:,0], data_all_test[:,2],c=data_label_test)

plt.scatter(data_all[:,0], data_all[:,2],c=data_label)

#creating clusters kmeans
number_of_clusters = 25* number_of_classes
KMeans()
clust= KMeans(n_clusters=number_of_clusters)
clust.fit(data_all_train)
#print label of clusters
print(clust.labels_)

#create array to group cluster intsnaces
clusters = np.array([])
for i in clust.labels_:
    clusters = np.append(clusters, [i, data_all_train[i]])

clusters

from collections import defaultdict

#creating list grouping idexes of training data grouped by cluster label
clusters_with_id = defaultdict(list)
for idx, cluster in enumerate(clust.labels_):
    clusters_with_id[cluster].append(idx)

clusters_with_id

def median_in_cluster(data_all, indexes):
    count_of_values = len(indexes)
    sum = 0

    # liczba wszystkich wymiarów wzgledem których ma być sumowane
    count_of_features = data_all[0].shape[0]

    median = np.array([])
    
    for feature in range(count_of_features):
        for index in indexes:
            actual_data = data_all[index]
            sum += actual_data[feature]
        median = np.append(median, sum/count_of_values)
        sum = 0

    return median

"""
Function finding id of point in data, nearest to given point. Using for find nerest point of mean in homogeniuos cluster 
TODO czy ma zwracać kilka indesów jeżeli takie same odległosci?
"""
def find_id_of_nearest_point(data_all, indexes, point):
    #id of nearest, for now the first
    id = indexes[0] 
    #minimal distance, for now - the first distance
    min_dist = distance.euclidean(point, data_all[id])

    for i in indexes:
        data = data_all[i]
        dist = distance.euclidean(point, data)
        if min_dist > dist:
            min_dist = dist
            id = i

    return id

"""
Function return index of nearest instance to given
element - index of instance 
indexes_of_data - indexes of instances from which have to find nearest to element
data_all - array of all instances to get data of selected index
"""
def  find_nearest_instance(element, indexes_of_data, data_all):
    point = data_all[element]
    #first temporary index
    id = 0
    #minimal distance, for now - the first distance
    min_dist = distance.euclidean(point, data_all[id])
    for i in indexes_of_data:
        if i == element:
            break
        data = data_all[i]
        dist = distance.euclidean(point, data)
        if min_dist > dist:
            min_dist = dist
            id = i

    return id

'''
data_all - data from getting exact
indexes - indexes of data from data_all
'''
def data_for_indexes(data_all, indexes):
    data_indexes = np.array([])
    for id in indexes:
        data_indexes = np.append(data_indexes, data_all[id])

    return data_indexes

"""
Function return index of majority class
"""
def  find_majority_class(number_of_classes, classes_with_indexes):
    max = len(classes_with_indexes[0])
    majority_class = 0

    for i in range(number_of_classes):
        count = len(classes_with_indexes[i])
        if max < count:
            max = count
            majority_class = i

    return majority_class

def mean_point_in_cluster(data_all, indexes):
    count_of_values = len(indexes)
    sum = 0

    # liczba wszystkich wymiarów wzgledem których ma być sumowane
    count_of_features = data_all[0].shape[0]

    mean_point = np.array([])
    
    for feature in range(count_of_features):
        sum = 0
        for index in indexes:
            actual_data = data_all[index]
            sum += actual_data[feature]
        mean_point = np.append(mean_point, sum/count_of_values)

    return mean_point

#clusters_with_id[key-czyli label of cluster :)]
#clusters_with_id[6]

#utowrzenie tablicy n wartości, gdzie n to liczba klas, w której bedzie trzymana liczba wystapień danej klasy
#initialize array with 0 occurrence of each class
classes_with_data = [] #zestaw instancji danych dla każdej z klasy tj np classes_data[0] - dane instancji klasy pierwszej w klastrze
classes_with_indexes = []
#create empty reduced dataset
reduced_set = []
reduced_data = []

classes_count = np.array([])
for i in range(number_of_classes):
    classes_count = np.append(classes_count, 0)
    classes_with_data.append([])
    classes_with_indexes.append([])
    reduced_set.append([])


#dla każdego klastra
#for each cluster
for i in range(number_of_clusters):
    #zliczamy liczbę wystąpień każdej z klas
    #dla każdego id wystepującej danej w klastrze sprawdzić klasę i zwiększyć liczbę wystąpień
    for instance_id in clusters_with_id[i]:
        class_label_of_instance = data_label_train[instance_id]
        #to teraz dodajemy do listy danych instancji tej klasy 
        #classes_data.append(data_all_train[instance_id])
        classes_with_data[class_label_of_instance].append(data_all_train[instance_id]) #dodajemy do
        #classes_with_data= np.append(classes_with_data[class_label_of_instance], data_all_train(instance_id))
        classes_with_indexes[class_label_of_instance].append(instance_id)
        classes_count[class_label_of_instance]+=1
    print('classes count:',classes_count)
    print('classes data with id:',classes_with_indexes)

    is_homogeniuos = True
    count_of_classes_in_cluster = 0
    for j in range(number_of_classes):
        if(classes_count[j] > 0):
            count_of_classes_in_cluster+=1
    if (count_of_classes_in_cluster > 1):
        is_homogeniuos = False


    if (is_homogeniuos):
        print('homogenious')
        cm = find_majority_class(number_of_classes, classes_with_indexes)
        #obliczyć średnia wszystkich wartości instancji w klastrze w postaci obiektu-
        #dla każdej instancji obliczyć odległość euklidesową od tej średniej i zapisać w jakiejś pomocniczej np.array?
        #wybrać 
        mean_point = mean_point_in_cluster(data_all=data_all_train, indexes=clusters_with_id[i])
        #print(mean_point)
        accept_id = find_id_of_nearest_point(data_all = data_all_train, indexes = clusters_with_id[i], point = mean_point)
        #print(accept_id)
        reduced_set[cm].append(data_all_train[accept_id])
        reduced_data.append(data_all_train[accept_id])
        
    else:
        """
        TODO 
        majority_class_label 
        for i in set of other class 
            for data with class 


        robimy liste labeli, sprawdzamy, która jest najczęsciej - ta zapamiętujemy i wywalamy z listy;
        dla każdej klasy z listy
            dla każdego elementu z tej klasy w tym klastrze
                szukamy elementu z klasy głowenj, który jest najbliżej tego - potrzebny tez indeks
                szukamy elemntu z tej klasy, który jest najbliżej tego znalezonego z głównej
        """
        #majority class
        cm = find_majority_class(number_of_classes, classes_with_indexes)
        print(cm)

        for class_id in range(number_of_classes):
            if class_id == cm:
                break
            for el in classes_with_indexes[class_id]:
                #najbliższy element do badanego spośród głownej klasy
                nearest_of_majority_class = find_nearest_instance(element = el, indexes_of_data = classes_with_indexes[cm], data_all = data_all_train)
                print('nearest_of_majority_class:',nearest_of_majority_class)
                #reduced_set = np.append(reduced_set, data_all_train[nearest_of_majority_class])
                reduced_set[cm].append(data_all_train[nearest_of_majority_class])
                reduced_data.append(data_all_train[nearest_of_majority_class])
                #najbliżsy element do badanego spośród tej samej klasy co badany
                nearest_of_actual_class = find_nearest_instance(element = el, indexes_of_data = classes_with_indexes[class_id], data_all = data_all_train)
                print('nearest_of_actual_class:', nearest_of_actual_class)
                #reduced_set = np.append(reduced_set, data_all_train[nearest_of_actual_class])
                reduced_set[cm].append(data_all_train[nearest_of_actual_class])
                reduced_data.append(data_all_train[nearest_of_actual_class])


        
    #reset classes counter     
    classes_count = np.array([])
    classes_with_data = [] #zestaw instancji danych dla każdej z klasy tj np classes_data[0] - dane instancji klasy pierwszej w klastrze
    classes_with_indexes = []
    for i in range(number_of_classes):
        classes_count = np.append(classes_count, 0)
        classes_with_data.append([])
        classes_with_indexes.append([])

reduced_set

#final labels
reduced_colors = []

for i in range(number_of_classes):
    for id in reduced_set[i]:
        reduced_colors.append(i)


colors = {0:'red',1:'green',2:'blue', 3: 'purple'}

#test
t = time.process_time()
#knn classify - measure time and accuracy for original train dataset
knn = KNeighborsClassifier(n_neighbors=5)
knn.fit(data_all_train,data_label_train)
accuracy = knn.score(data_all_test,data_label_test)
elapsed = time.process_time() - t

print("Time:  ", elapsed)
print('Accuracy:  ', accuracy)
print('Count of instances', len(data_all_train))

t = time.process_time()
knn2 = KNeighborsClassifier(n_neighbors=5)
knn2.fit(np.array(reduced_data),np.array(reduced_colors))
accuracy = knn2.score(data_all_test,data_label_test)
elapsed = time.process_time() - t

print("Time:  ", elapsed)
print('Accuracy:  ', accuracy)
print('Count of instances', len(reduced_data))

plt.scatter(data_all_train[:,0], data_all_train[:,2],c=data_label_train)

data_all_train.shape

np_red_data = np.array(reduced_data)

data_label_train.shape

np_red_col = np.array(reduced_colors)

plt.scatter(np_red_data[:,0], np_red_data[:,2],c=np_red_col)


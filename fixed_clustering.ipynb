{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.datasets import fetch_rcv1\n",
    "from sklearn import datasets\n",
    "from scipy.spatial import distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "train = pd.read_csv('D:\\Studia\\inz\\datasets\\shuttle\\shuttle_trn\\shuttle_trn.csv', sep = ' ')#, header=None)\n",
    "train = train.iloc[:, :10]\n",
    "train.to_csv('D:\\Studia\\inz\\datasets\\shuttle\\shuttle_training.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "PermissionError",
     "evalue": "[Errno 13] Permission denied: 'D:\\\\Studia\\\\inz\\\\datasets\\\\shuttle\\\\shuttle_test.csv'",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPermissionError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-25-a7326422709d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtrain\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'D:\\Studia\\inz\\datasets\\shuttle\\shuttle_test.csv'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindex\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36mto_csv\u001b[1;34m(self, path_or_buf, sep, na_rep, float_format, columns, header, index, index_label, mode, encoding, compression, quoting, quotechar, line_terminator, chunksize, date_format, doublequote, escapechar, decimal)\u001b[0m\n\u001b[0;32m   3202\u001b[0m             \u001b[0mdecimal\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdecimal\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3203\u001b[0m         )\n\u001b[1;32m-> 3204\u001b[1;33m         \u001b[0mformatter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3205\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3206\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mpath_or_buf\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\pandas\\io\\formats\\csvs.py\u001b[0m in \u001b[0;36msave\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    186\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    187\u001b[0m                 \u001b[0mencoding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mencoding\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 188\u001b[1;33m                 \u001b[0mcompression\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompression_args\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompression\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    189\u001b[0m             )\n\u001b[0;32m    190\u001b[0m             \u001b[0mclose\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\pandas\\io\\common.py\u001b[0m in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text)\u001b[0m\n\u001b[0;32m    426\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mencoding\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    427\u001b[0m             \u001b[1;31m# Encoding\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 428\u001b[1;33m             \u001b[0mf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath_or_buf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mencoding\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnewline\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    429\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0mis_text\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    430\u001b[0m             \u001b[1;31m# No explicit encoding\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mPermissionError\u001b[0m: [Errno 13] Permission denied: 'D:\\\\Studia\\\\inz\\\\datasets\\\\shuttle\\\\shuttle_test.csv'"
     ]
    }
   ],
   "source": [
    "train.to_csv('D:\\Studia\\inz\\datasets\\shuttle\\shuttle_test.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "       Rad_Flow  Fpv_Close  Fpv_Open  High  Bypass  Bpv_Close  Bpv_Open   1  \\\n0            50         21        77     0      28          0        27  48   \n1            55          0        92     0       0         26        36  92   \n2            53          0        82     0      52         -5        29  30   \n3            37          0        76     0      28         18        40  48   \n4            37          0        79     0      34        -26        43  46   \n...         ...        ...       ...   ...     ...        ...       ...  ..   \n43495        46          5        78     0      46          5        32  32   \n43496        37          0        79    -1      10          3        43  69   \n43497        48          0        78     3      46          0        30  32   \n43498        41          0        79     0      38        -25        38  40   \n43499        40         -3       100     0      38          0        61  62   \n\n        2  class  \n0      22      2  \n1      56      4  \n2       2      1  \n3       8      1  \n4       2      1  \n...    ..    ...  \n43495   0      1  \n43496  26      1  \n43497   2      1  \n43498   2      1  \n43499   2      1  \n\n[43500 rows x 10 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Rad_Flow</th>\n      <th>Fpv_Close</th>\n      <th>Fpv_Open</th>\n      <th>High</th>\n      <th>Bypass</th>\n      <th>Bpv_Close</th>\n      <th>Bpv_Open</th>\n      <th>1</th>\n      <th>2</th>\n      <th>class</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>50</td>\n      <td>21</td>\n      <td>77</td>\n      <td>0</td>\n      <td>28</td>\n      <td>0</td>\n      <td>27</td>\n      <td>48</td>\n      <td>22</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>55</td>\n      <td>0</td>\n      <td>92</td>\n      <td>0</td>\n      <td>0</td>\n      <td>26</td>\n      <td>36</td>\n      <td>92</td>\n      <td>56</td>\n      <td>4</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>53</td>\n      <td>0</td>\n      <td>82</td>\n      <td>0</td>\n      <td>52</td>\n      <td>-5</td>\n      <td>29</td>\n      <td>30</td>\n      <td>2</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>37</td>\n      <td>0</td>\n      <td>76</td>\n      <td>0</td>\n      <td>28</td>\n      <td>18</td>\n      <td>40</td>\n      <td>48</td>\n      <td>8</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>37</td>\n      <td>0</td>\n      <td>79</td>\n      <td>0</td>\n      <td>34</td>\n      <td>-26</td>\n      <td>43</td>\n      <td>46</td>\n      <td>2</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>43495</th>\n      <td>46</td>\n      <td>5</td>\n      <td>78</td>\n      <td>0</td>\n      <td>46</td>\n      <td>5</td>\n      <td>32</td>\n      <td>32</td>\n      <td>0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>43496</th>\n      <td>37</td>\n      <td>0</td>\n      <td>79</td>\n      <td>-1</td>\n      <td>10</td>\n      <td>3</td>\n      <td>43</td>\n      <td>69</td>\n      <td>26</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>43497</th>\n      <td>48</td>\n      <td>0</td>\n      <td>78</td>\n      <td>3</td>\n      <td>46</td>\n      <td>0</td>\n      <td>30</td>\n      <td>32</td>\n      <td>2</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>43498</th>\n      <td>41</td>\n      <td>0</td>\n      <td>79</td>\n      <td>0</td>\n      <td>38</td>\n      <td>-25</td>\n      <td>38</td>\n      <td>40</td>\n      <td>2</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>43499</th>\n      <td>40</td>\n      <td>-3</td>\n      <td>100</td>\n      <td>0</td>\n      <td>38</td>\n      <td>0</td>\n      <td>61</td>\n      <td>62</td>\n      <td>2</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n<p>43500 rows × 10 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 33
    }
   ],
   "source": [
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "        0  1    2  3   4   5   6    7    8  9\n0      55  0   81  0  -6  11  25   88   64  4\n1      56  0   96  0  52  -4  40   44    4  4\n2      50 -1   89 -7  50   0  39   40    2  1\n3      53  9   79  0  42  -2  25   37   12  4\n4      55  2   82  0  54  -6  26   28    2  1\n...    .. ..  ... ..  ..  ..  ..  ...  ... ..\n14495  80  0   84  0 -36 -29   4  120  116  5\n14496  55  0   81  0 -20  25  26  102   76  4\n14497  55  0   77  0  12 -22  22   65   42  4\n14498  37  0  103  0  18 -16  66   85   20  1\n14499  56  2   98  0  52   1  42   46    4  4\n\n[14500 rows x 10 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>0</th>\n      <th>1</th>\n      <th>2</th>\n      <th>3</th>\n      <th>4</th>\n      <th>5</th>\n      <th>6</th>\n      <th>7</th>\n      <th>8</th>\n      <th>9</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>55</td>\n      <td>0</td>\n      <td>81</td>\n      <td>0</td>\n      <td>-6</td>\n      <td>11</td>\n      <td>25</td>\n      <td>88</td>\n      <td>64</td>\n      <td>4</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>56</td>\n      <td>0</td>\n      <td>96</td>\n      <td>0</td>\n      <td>52</td>\n      <td>-4</td>\n      <td>40</td>\n      <td>44</td>\n      <td>4</td>\n      <td>4</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>50</td>\n      <td>-1</td>\n      <td>89</td>\n      <td>-7</td>\n      <td>50</td>\n      <td>0</td>\n      <td>39</td>\n      <td>40</td>\n      <td>2</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>53</td>\n      <td>9</td>\n      <td>79</td>\n      <td>0</td>\n      <td>42</td>\n      <td>-2</td>\n      <td>25</td>\n      <td>37</td>\n      <td>12</td>\n      <td>4</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>55</td>\n      <td>2</td>\n      <td>82</td>\n      <td>0</td>\n      <td>54</td>\n      <td>-6</td>\n      <td>26</td>\n      <td>28</td>\n      <td>2</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>14495</th>\n      <td>80</td>\n      <td>0</td>\n      <td>84</td>\n      <td>0</td>\n      <td>-36</td>\n      <td>-29</td>\n      <td>4</td>\n      <td>120</td>\n      <td>116</td>\n      <td>5</td>\n    </tr>\n    <tr>\n      <th>14496</th>\n      <td>55</td>\n      <td>0</td>\n      <td>81</td>\n      <td>0</td>\n      <td>-20</td>\n      <td>25</td>\n      <td>26</td>\n      <td>102</td>\n      <td>76</td>\n      <td>4</td>\n    </tr>\n    <tr>\n      <th>14497</th>\n      <td>55</td>\n      <td>0</td>\n      <td>77</td>\n      <td>0</td>\n      <td>12</td>\n      <td>-22</td>\n      <td>22</td>\n      <td>65</td>\n      <td>42</td>\n      <td>4</td>\n    </tr>\n    <tr>\n      <th>14498</th>\n      <td>37</td>\n      <td>0</td>\n      <td>103</td>\n      <td>0</td>\n      <td>18</td>\n      <td>-16</td>\n      <td>66</td>\n      <td>85</td>\n      <td>20</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>14499</th>\n      <td>56</td>\n      <td>2</td>\n      <td>98</td>\n      <td>0</td>\n      <td>52</td>\n      <td>1</td>\n      <td>42</td>\n      <td>46</td>\n      <td>4</td>\n      <td>4</td>\n    </tr>\n  </tbody>\n</table>\n<p>14500 rows × 10 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 34
    }
   ],
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('D:\\Studia\\inz\\datasets\\shuttle\\shuttle_training.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_csv('D:\\Studia\\inz\\datasets\\shuttle\\shuttle_test.csv')#, header = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "Index(['Rad_Flow', 'Fpv_Close', 'Fpv_Open', 'High', 'Bypass', 'Bpv_Close',\n       'Bpv_Open', '1', '2', 'class'],\n      dtype='object')"
     },
     "metadata": {},
     "execution_count": 40
    }
   ],
   "source": [
    "train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "       Rad_Flow  Fpv_Close  Fpv_Open  High  Bypass  Bpv_Close  Bpv_Open    1  \\\n0            50         21        77     0      28          0        27   48   \n1            55          0        92     0       0         26        36   92   \n2            53          0        82     0      52         -5        29   30   \n3            37          0        76     0      28         18        40   48   \n4            37          0        79     0      34        -26        43   46   \n...         ...        ...       ...   ...     ...        ...       ...  ...   \n14495        80          0        84     0     -36        -29         4  120   \n14496        55          0        81     0     -20         25        26  102   \n14497        55          0        77     0      12        -22        22   65   \n14498        37          0       103     0      18        -16        66   85   \n14499        56          2        98     0      52          1        42   46   \n\n         2  class  \n0       22      2  \n1       56      4  \n2        2      1  \n3        8      1  \n4        2      1  \n...    ...    ...  \n14495  116      5  \n14496   76      4  \n14497   42      4  \n14498   20      1  \n14499    4      4  \n\n[58000 rows x 10 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Rad_Flow</th>\n      <th>Fpv_Close</th>\n      <th>Fpv_Open</th>\n      <th>High</th>\n      <th>Bypass</th>\n      <th>Bpv_Close</th>\n      <th>Bpv_Open</th>\n      <th>1</th>\n      <th>2</th>\n      <th>class</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>50</td>\n      <td>21</td>\n      <td>77</td>\n      <td>0</td>\n      <td>28</td>\n      <td>0</td>\n      <td>27</td>\n      <td>48</td>\n      <td>22</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>55</td>\n      <td>0</td>\n      <td>92</td>\n      <td>0</td>\n      <td>0</td>\n      <td>26</td>\n      <td>36</td>\n      <td>92</td>\n      <td>56</td>\n      <td>4</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>53</td>\n      <td>0</td>\n      <td>82</td>\n      <td>0</td>\n      <td>52</td>\n      <td>-5</td>\n      <td>29</td>\n      <td>30</td>\n      <td>2</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>37</td>\n      <td>0</td>\n      <td>76</td>\n      <td>0</td>\n      <td>28</td>\n      <td>18</td>\n      <td>40</td>\n      <td>48</td>\n      <td>8</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>37</td>\n      <td>0</td>\n      <td>79</td>\n      <td>0</td>\n      <td>34</td>\n      <td>-26</td>\n      <td>43</td>\n      <td>46</td>\n      <td>2</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>14495</th>\n      <td>80</td>\n      <td>0</td>\n      <td>84</td>\n      <td>0</td>\n      <td>-36</td>\n      <td>-29</td>\n      <td>4</td>\n      <td>120</td>\n      <td>116</td>\n      <td>5</td>\n    </tr>\n    <tr>\n      <th>14496</th>\n      <td>55</td>\n      <td>0</td>\n      <td>81</td>\n      <td>0</td>\n      <td>-20</td>\n      <td>25</td>\n      <td>26</td>\n      <td>102</td>\n      <td>76</td>\n      <td>4</td>\n    </tr>\n    <tr>\n      <th>14497</th>\n      <td>55</td>\n      <td>0</td>\n      <td>77</td>\n      <td>0</td>\n      <td>12</td>\n      <td>-22</td>\n      <td>22</td>\n      <td>65</td>\n      <td>42</td>\n      <td>4</td>\n    </tr>\n    <tr>\n      <th>14498</th>\n      <td>37</td>\n      <td>0</td>\n      <td>103</td>\n      <td>0</td>\n      <td>18</td>\n      <td>-16</td>\n      <td>66</td>\n      <td>85</td>\n      <td>20</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>14499</th>\n      <td>56</td>\n      <td>2</td>\n      <td>98</td>\n      <td>0</td>\n      <td>52</td>\n      <td>1</td>\n      <td>42</td>\n      <td>46</td>\n      <td>4</td>\n      <td>4</td>\n    </tr>\n  </tbody>\n</table>\n<p>58000 rows × 10 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 46
    }
   ],
   "source": [
    "pd.concat([train, test]).to_csv('D:\\Studia\\inz\\datasets\\shuttle\\shuttle_all.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler().fit(x)\n",
    "x = scaler.transform(x)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 527,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "6.345076831686122"
     },
     "metadata": {},
     "execution_count": 527
    }
   ],
   "source": [
    "np.sqrt(sum(x[0]**2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 530,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    def n_normalize(array: np.ndarray):\n",
    "        \"\"\"Function normalizes numpy array.\n",
    "\n",
    "        Args:\n",
    "            array (np.ndarray): array to normalize\n",
    "\n",
    "        Returns:\n",
    "            normalized (np.ndarray): normalized :array\n",
    "            weights (np.ndarray): weights of normalization\n",
    "        \"\"\"\n",
    "        normalized = array\n",
    "        weights = np.ones(array.shape[1])\n",
    "        for i in range(len(weights)):\n",
    "            weights[i] = np.sqrt(sum(array[i]**2))\n",
    "        for i in range(len(weights)): \n",
    "            normalized[:,i] /= weights[i]\n",
    "        return normalized, weights\n",
    "\n",
    " \n",
    "    def reverse_normalize(array: np.ndarray, weights: np.ndarray):\n",
    "        \"\"\"Function reverse normalizes numpy array - back to values before normalization\n",
    "\n",
    "        Raises:\n",
    "            Exception: when number of weights and columns does not agree\n",
    "\n",
    "        Returns:\n",
    "            (np.ndarray): array with values before normalization\n",
    "        \"\"\"\n",
    "        if array.shape[1] != len(weights):\n",
    "            raise Exception('Number of weights does not agree with shape[1] of array')\n",
    "        arr_before = array\n",
    "        for i in range(len(weights)): \n",
    "            arr_before[:,i] *= weights[i]\n",
    "        return arr_before"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 528,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "array([72.27620632, 37.77631533, 50.82322304, 17.38677658])"
     },
     "metadata": {},
     "execution_count": 528
    }
   ],
   "source": [
    "weights = np.sqrt(sum(x**2))\n",
    "weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 533,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "array([1.03054727, 0.95559481, 0.94756828, 0.93524825])"
     },
     "metadata": {},
     "execution_count": 533
    }
   ],
   "source": [
    "x,c = n_normalize(x)\n",
    "c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(w)): \n",
    "    x[:,i] = x[:,i]w[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x*2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.array(pdd_data)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xn/val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unique(list): \n",
    "    '''\n",
    "    Creates list with unique values from list\n",
    "    '''\n",
    "    # intilize a null list \n",
    "    unique_list = []\n",
    "    # traverse for all elements \n",
    "    for x in list: \n",
    "        # check if exists in unique_list or not \n",
    "        if x not in unique_list: \n",
    "            unique_list.append(x) \n",
    "\n",
    "    return unique_list\n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "data_all, data_label = datasets.load_iris(return_X_y=True) #datasets.load_breast_cancer(return_X_y=True) #datasets.load_iris(return_X_y=True)\n",
    "data_all_train, data_all_test, data_label_train, data_label_test = train_test_split(data_all, data_label, test_size=0.3)\n",
    "\n",
    "number_of_classes = len(unique(data_label))\n",
    "print(number_of_classes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import normalize\n",
    "\n",
    "\n",
    "\n",
    "data_all= pdd_data\n",
    "data_label = pdd_label\n",
    "#map labels to 0-n indexes\n",
    "class_dict = dict()\n",
    "i=0\n",
    "for label in set(data_label):\n",
    "    class_dict[label] = i\n",
    "    i+=1\n",
    "data_all_train, data_all_test, data_label_train, data_label_test = train_test_split(data_all, data_label, test_size=0.3)\n",
    "number_of_classes = len(unique(data_label))\n",
    "print(number_of_classes)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert to numpy\n",
    "data_all= np.array(data_all)\n",
    "data_label = np.array(data_label)\n",
    "data_all_train = np.array(data_all_train)\n",
    "data_all_test=np.array(data_all_test) \n",
    "data_label_train = np.array(data_label_train)\n",
    "data_label_test = np.array(data_label_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_all_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdd = pd.read_csv(\"D:\\Studia\\inz\\Repos\\DataReduction\\InstanceReduction\\datasets_csv\\iris.csv\")#D:\\Studia\\inz\\glass\\Titanic.csv\", sep = \",\") #winequality-red.csv\", \n",
    "pdd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#testing loading data and encode columns with cathegorial values \n",
    "#pdd = pd.read_csv(\"D:\\Studia\\inz\\datasets\\satimage.csv\")#D:\\Studia\\inz\\glass\\Titanic.csv\", sep = \",\") #winequality-red.csv\", sep = \";\") \n",
    "#(\"D:\\Studia\\inz\\datasets\\iris\\iris_data.csv\")\n",
    "from sklearn import preprocessing as prp\n",
    "\n",
    "#column with labels, index or column name\n",
    "label_col = 'class'#\"quality\"#4\n",
    "\n",
    "# encoder = prp.LabelEncoder()\n",
    "# n_features = pdd.shape[1]\n",
    "\n",
    "# for i in range(n_features):\n",
    "#     print(pdd.dtypes[i])\n",
    "#     if(pdd.dtypes[i] == 'object'):\n",
    "#         print('obj')\n",
    "#         encoder.fit(pdd.iloc[:, [i]])\n",
    "#         print(encoder.classes_)\n",
    "        \n",
    "#         #pdd.replace(pdd.iloc[:, [i]], encoder.transform(encoder.classes_) )\n",
    "#         # apply le on categorical feature columns\n",
    "#         #pdd[i] = pdd[i].apply(lambda col: encoder.fit_transform(pdd.iloc[:, [i]]))\n",
    "\n",
    "# categorical type filter\n",
    "categorical_features = pdd.dtypes==object\n",
    "# filter categorical column\n",
    "categorical_cols = pdd.columns[categorical_features].tolist()\n",
    "encoder = prp.LabelEncoder()\n",
    "# apply encoder on categorical feature columns\n",
    "try:\n",
    "    pdd[categorical_cols] = pdd[categorical_cols].apply(lambda col: encoder.fit_transform(col))\n",
    "except ValueError:\n",
    "    print(\"Nothing to encode\")\n",
    "except TypeError: \n",
    "    print(\"TypeError: '<' not supported between instances of 'str' and 'float' During handling of the above exception, another exception occurred:\")\n",
    "########################\n",
    "if type(label_col)==str: #column name\n",
    "    #create data frame with class labels\n",
    "    pdd_label = pdd[label_col]\n",
    "    #drop column with label \n",
    "    pdd_data = pdd.drop(columns=label_col)\n",
    "else: #column index\n",
    "    #create data frame with class labels\n",
    "    pdd_label = pdd[label_col]\n",
    "    #drop column with label \n",
    "    pdd_data = pdd.drop(pdd.columns[label_col], axis = 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_all.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x= plt.scatter(data_all_train[:,0], data_all_train[:,3],c=data_label_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.xlim(x.get_x_lim, )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(data_all_train[:,0], data_all_train[:,2],c=data_label_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(data_all_test[:,0], data_all_test[:,2],c=data_label_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(data_all[:,0], data_all[:,2],c=data_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import SpectralClustering #działało ale nie radzi sobie w większej ilości + musi być parzysta liczba klastrów\n",
    "from sklearn.cluster import AgglomerativeClustering #działa ok\n",
    "from sklearn.cluster import DBSCAN #zerodivision\n",
    "from sklearn.cluster import OPTICS #zerodivision - -1 one cluster\n",
    "import sklearn.cluster as clus\n",
    "# grouping_types_with_ncl = { kmeans: KMeans(n_clusters=number_of_clusters),\n",
    "#                         spectral: SpectralClustering(n_clusters=number_of_clusters),\n",
    "#                         agglomerative: Agglomerative(n_clusters=number_of_clusters)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_clusters(data, number_of_clusters = 20):\n",
    "    '''\n",
    "    Function creating n_clusters from data\n",
    "    Return array of labels of created clusters.\n",
    "    '''\n",
    "\n",
    "    #creating clusters using k-means algorithm\n",
    "    clust = KMeans(n_clusters=number_of_clusters)#AgglomerativeClustering(n_clusters=number_of_clusters)#\n",
    "    clust.fit(data)\n",
    "    \n",
    "    return clust.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def group_id_by_cluster(clusters):\n",
    "    '''\n",
    "    Function grouping indexes of data by indexes of clusters\n",
    "    Return dictionary\n",
    "    '''\n",
    "    #creating list grouping idexes of training data grouped by cluster label\n",
    "    clusters_with_id = defaultdict(list)\n",
    "    for idx, cluster in enumerate(clusters):\n",
    "        clusters_with_id[cluster].append(idx)\n",
    "\n",
    "    return clusters_with_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "#create clusters\n",
    "number_of_clusters = 10* number_of_classes\n",
    "clusters = create_clusters(data = data_all_train, number_of_clusters = number_of_clusters) \n",
    "clusters_with_id = group_id_by_cluster(clusters = clusters)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array(set(data_label_train))\n",
    "data_label_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xd.most_common(1)[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "xd = Counter(x[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.array([['A', 'A', 'B']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def median_in_cluster(data_all, indexes):\n",
    "#     count_of_values = len(indexes)\n",
    "#     sum = 0\n",
    "\n",
    "#     # liczba wszystkich wymiarów wzgledem których ma być sumowane\n",
    "#     count_of_features = data_all[0].shape[0]\n",
    "\n",
    "#     median = np.array([])\n",
    "    \n",
    "#     for feature in range(count_of_features):\n",
    "#         for index in indexes:\n",
    "#             actual_data = data_all[index]\n",
    "#             sum += actual_data[feature]\n",
    "#         median = np.append(median, sum/count_of_values)\n",
    "#         sum = 0\n",
    "\n",
    "#     return median"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Function finding index of point in data, nearest to given point. Using for find nerest point of mean in homogeniuos cluster \n",
    "TODO czy ma zwracać kilka indesów jeżeli takie same odległosci?\n",
    "\"\"\"\n",
    "def find_id_of_nearest_point(data_all, indexes, point):\n",
    "    #id of nearest, for now the first\n",
    "    id = indexes[0] \n",
    "    #minimal distance, for now - the first distance\n",
    "    min_dist = distance.euclidean(point, data_all[id])\n",
    "\n",
    "    for i in indexes:\n",
    "        data = data_all[i]\n",
    "        dist = distance.euclidean(point, data)\n",
    "        if min_dist > dist:\n",
    "            min_dist = dist\n",
    "            id = i\n",
    "\n",
    "    return id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def find_nearest_instance(element, indexes_of_data, data_all):\n",
    "    \"\"\"\n",
    "    Function return index of nearest instance to given\n",
    "    element - index of instance \n",
    "    indexes_of_data - indexes of instances from which have to find nearest to element\n",
    "    data_all - array of all instances to get data of selected index\n",
    "    \"\"\"\n",
    "    point = data_all[element]\n",
    "    #first temporary index\n",
    "    id = 0\n",
    "    #minimal distance, for now - the first distance\n",
    "    min_dist = distance.euclidean(point, data_all[id])\n",
    "    for i in indexes_of_data:\n",
    "        if i == element:\n",
    "            break\n",
    "        data = data_all[i]\n",
    "        dist = distance.euclidean(point, data)\n",
    "        if min_dist > dist:\n",
    "            min_dist = dist\n",
    "            id = i\n",
    "\n",
    "    return id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# '''\n",
    "# data_all - data from getting exact\n",
    "# indexes - indexes of data from data_all\n",
    "# '''\n",
    "# def data_for_indexes(data_all, indexes):\n",
    "#     data_indexes = np.array([])\n",
    "#     for id in indexes:\n",
    "#         data_indexes = np.append(data_indexes, data_all[id])\n",
    "\n",
    "#     return data_indexes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def find_majority_class(number_of_classes, classes_with_indexes):\n",
    "    \"\"\"\n",
    "    Function return index of majority class\n",
    "    \"\"\"\n",
    "    max = len(classes_with_indexes[0])\n",
    "    majority_class = 0\n",
    "\n",
    "    for i in range(number_of_classes):\n",
    "        count = len(classes_with_indexes[i])\n",
    "        if max < count:\n",
    "            max = count\n",
    "            majority_class = i\n",
    "\n",
    "    return majority_class\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def mean_point_in_cluster(data_all, indexes):\n",
    "    \"\"\"\n",
    "    Function calculating mean point in cluster.\n",
    "    data_all - training dataset\n",
    "    indexes - array of indexes of cluster form training dataset\n",
    "\n",
    "    TODO: ZeroDivisionException\n",
    "    \"\"\"\n",
    "    count_of_values = len(indexes)\n",
    "    sum = 0\n",
    "\n",
    "    #dimesionality of point\n",
    "    count_of_features = data_all[0].shape[0]\n",
    "    mean_point = np.array([])\n",
    "    \n",
    "    for feature in range(count_of_features):\n",
    "        sum = 0\n",
    "        for index in indexes:\n",
    "            actual_data = data_all[index]\n",
    "            sum += actual_data[feature]\n",
    "        try:\n",
    "            mean_point = np.append(mean_point, sum/count_of_values)\n",
    "        except ZeroDivisionException:\n",
    "            print('Can not division by 0!')\n",
    "\n",
    "    return mean_point\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def group_cluster_by_class(cluster):\n",
    "    '''\n",
    "    Function creates array with indexes in cluster grouped by class label\n",
    "    '''\n",
    "    #initialize array with 0 occurrence of each class\n",
    "    classes_with_indexes = []\n",
    "\n",
    "    #initialize array\n",
    "    for i in range(number_of_classes):\n",
    "        classes_with_indexes.append([])\n",
    "\n",
    "    for instance_id in cluster:\n",
    "    #checking label of instance\n",
    "        class_label_of_instance = data_label_train[instance_id]\n",
    "        #add to array for class label\n",
    "        classes_with_indexes[class_dict[class_label_of_instance]].append(instance_id)\n",
    "\n",
    "    return classes_with_indexes\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_homogenious(cluster):\n",
    "    '''\n",
    "    Function checking if the cluster is homogenious or not\n",
    "    Return True if is, False if not.\n",
    "    '''\n",
    "    grouped_cluster = group_cluster_by_class(cluster)\n",
    "    \n",
    "    is_homogeniuos = True\n",
    "    count_of_classes_in_cluster = 0\n",
    "    for i in range(number_of_classes):\n",
    "        if(len(grouped_cluster[i]) > 0):\n",
    "            count_of_classes_in_cluster+=1\n",
    "    if (count_of_classes_in_cluster > 1):\n",
    "        is_homogeniuos = False\n",
    "    \n",
    "    return is_homogeniuos\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_reduced_set(reduced_set):\n",
    "    '''\n",
    "    Function prepare reduced dataset grouped by label for using in classificators\n",
    "    reduced_set - dataset grouped by label\n",
    "\n",
    "    Return:\n",
    "    np_red_data - uninterrupted array of instances\n",
    "    np_red_label - array of labels\n",
    "\n",
    "    TODO: remove repeated values?\n",
    "    '''\n",
    "    \n",
    "    reduced_labels = []\n",
    "    tmp = []\n",
    "    for i in range(number_of_classes):\n",
    "        for id in reduced_set[i]:\n",
    "            reduced_labels.append(list(class_dict)[i])\n",
    "            tmp.append(id.tolist())\n",
    "\n",
    "    np_red_data = np.array(tmp)\n",
    "    np_red_label = np.array(reduced_labels)\n",
    "    \n",
    "    return np_red_data, np_red_label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clustering_reduction(clusters_with_id, data_all_train):\n",
    "    \"\"\"\n",
    "    The main function of clustering reduction module\n",
    "    \n",
    "    :param clusters_with_id: - indexes of instances from training dataset grouped by indexes of clusters\n",
    "    :param data_all_train: - training dataset\n",
    "\n",
    "    :returns: np_red_data - reduced dataset received as a result\n",
    "    np_red_col - labels of reduced dataset\n",
    "\n",
    "    \"\"\"\n",
    "    classes_with_indexes = []\n",
    "    #create empty reduced dataset\n",
    "    reduced_set = []\n",
    "\n",
    "    #init arrays dimensionality \n",
    "    for i in range(number_of_classes):\n",
    "        classes_with_indexes.append([])\n",
    "        reduced_set.append([])\n",
    "\n",
    "    #for each cluster\n",
    "    for i in range(number_of_clusters):\n",
    "        #for each instance in cluster\n",
    "        for instance_id in clusters_with_id[i]:\n",
    "            class_label_of_instance = data_label_train[instance_id]\n",
    "            classes_with_indexes[class_dict[class_label_of_instance]].append(instance_id)\n",
    "\n",
    "        #checking if the cluster is homogenious\n",
    "        is_homogeniuos = check_homogenious(clusters_with_id[i])\n",
    "\n",
    "\n",
    "        if (is_homogeniuos):\n",
    "            #find index of majority class - in this case only one possible\n",
    "            cm = find_majority_class(number_of_classes, classes_with_indexes)\n",
    "            #find mean point in cluster \n",
    "            mean_point = mean_point_in_cluster(data_all=data_all_train, indexes=clusters_with_id[i])\n",
    "            #print(mean_point)\n",
    "            #find index of intance located in cluster nearest to mean point\n",
    "            accept_id = find_id_of_nearest_point(data_all = data_all_train, indexes = clusters_with_id[i], point = mean_point)\n",
    "            #print(accept_id)\n",
    "\n",
    "            #add instance within the class 9to reduced set \n",
    "            reduced_set[cm].append(data_all_train[accept_id])\n",
    "            \n",
    "        else:\n",
    "            #majority class in cluster\n",
    "            cm = find_majority_class(number_of_classes, classes_with_indexes)\n",
    "            #print(cm)\n",
    "\n",
    "            #for each instance in other classes find nearest instance to checked from majority class and belonging class\n",
    "            #add instances to reduced set\n",
    "            for class_id in range(number_of_classes):\n",
    "                if class_id == cm:\n",
    "                    break\n",
    "                for el in classes_with_indexes[class_id]:\n",
    "                    #nearest form majority class\n",
    "                    nearest_of_majority_class = find_nearest_instance(element = el, indexes_of_data = classes_with_indexes[cm], data_all = data_all_train)\n",
    "                    reduced_set[cm].append(data_all_train[nearest_of_majority_class])\n",
    "                    #nearest from belonging class\n",
    "                    nearest_of_actual_class = find_nearest_instance(element = el, indexes_of_data = classes_with_indexes[class_id], data_all = data_all_train)\n",
    "                    #reduced_set = np.append(reduced_set, data_all_train[nearest_of_actual_class])\n",
    "                    reduced_set[cm].append(data_all_train[nearest_of_actual_class])\n",
    "            \n",
    "        #reset array    \n",
    "        classes_with_indexes = []\n",
    "        for j in range(number_of_classes):\n",
    "            classes_with_indexes.append([])\n",
    "\n",
    "    np_red_data, np_red_col = prepare_reduced_set(reduced_set)\n",
    "    return np_red_data, np_red_col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [],
   "source": [
    "np_red_data, np_red_col = clustering_reduction(clusters_with_id, data_all_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = pdd.columns.values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = list(class_dict.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(data_all_train[:,0], data_all_train[:,1],c=data_label_train, label = labels)\n",
    "plt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "scatter = ax.scatter(data_all_train[:,0], data_all_train[:,1],c=data_label_train)\n",
    "legend1 = ax.legend(*scatter.legend_elements(), title=\"Classes\")\n",
    "ax.add_artist(legend1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(np_red_data[:,0], np_red_data[:,1],c=np_red_col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_raport(time, accuracy, n_instances):\n",
    "    \"\"\"\n",
    "    Function prints raport\n",
    "    :param time: elapsed time\n",
    "    :param accuracy: accuracy of classifier\n",
    "    :param n_instances: number of instances in fited dataset\n",
    "    \"\"\"\n",
    "    print(\"Time:  \", time)\n",
    "    print('Accuracy:  ', accuracy)\n",
    "    print('Count of instances', n_instances)\n",
    "    print('=============')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "import time\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "\n",
    "classifiers = {'knn': KNeighborsClassifier(), \n",
    "                'svm': svm.SVC(), \n",
    "                'naive_bayers': GaussianNB(), \n",
    "                'decision_tree': DecisionTreeClassifier(),\n",
    "                'neutral_network': MLPClassifier()}\n",
    "\n",
    "\n",
    "# def raport_classify(original_set, original_labels, reduced_set, reduced_labels, test_set, test_labels, classifier_type = 'all'):\n",
    "#     '''\n",
    "#     Function creates raport for passed classifier_type. \n",
    "#     classifier_type - optional, if not passed raport is creating for all classifiers \n",
    "#     original_set - original dataset\n",
    "#     original_labels - array of labels for original dataset\n",
    "#     reduced_set - dataset gained after using reduction algorithm \n",
    "#     reduced_labels - array of labels for reduced_set\n",
    "#     '''\n",
    "#     if classifier_type == 'all':\n",
    "#         for c_type in classifiers:\n",
    "#             t = time.process_time()\n",
    "#             classifier = classifiers[c_type]\n",
    "#             classifier.fit(original_set, original_labels)\n",
    "#             accuracy = classifier.score(test_set, test_labels)\n",
    "#             elapsed = time.process_time() - t\n",
    "#             print('=============')\n",
    "#             print(\"Classifier:  \", c_type)\n",
    "#             print('=============')\n",
    "#             print(\"Raport for original dataset:\")\n",
    "#             print_raport(elapsed, accuracy, len(original_labels))\n",
    "\n",
    "#             t = time.process_time()\n",
    "#             classifier = classifiers[c_type]\n",
    "#             classifier.fit(reduced_set, reduced_labels)\n",
    "#             accuracy = classifier.score(test_set, test_labels)\n",
    "#             elapsed = time.process_time() - t\n",
    "#             print(\"Raport for reduced dataset:\")\n",
    "#             print_raport(elapsed, accuracy, len(reduced_labels))\n",
    "#     else:\n",
    "#         t = time.process_time()\n",
    "#         classifier = classifiers[classifier_type]\n",
    "#         classifier.fit(original_set, original_labels)\n",
    "#         accuracy = classifier.score(test_set, test_labels)\n",
    "#         elapsed = time.process_time() - t\n",
    "#         print('=============')\n",
    "#         print(\"Classifier:  \", classifier_type)\n",
    "#         print('=============')\n",
    "#         print(\"Raport for original dataset:\")\n",
    "#         print_raport(elapsed, accuracy, len(original_labels))\n",
    "\n",
    "#         t = time.process_time()\n",
    "#         classifier = classifiers[classifier_type]\n",
    "#         classifier.fit(reduced_set, reduced_labels)\n",
    "#         accuracy = classifier.score(test_set, test_labels)\n",
    "#         elapsed = time.process_time() - t\n",
    "#         print(\"Raport for reduced dataset:\")\n",
    "#         print_raport(elapsed, accuracy, len(reduced_labels))\n",
    "\n",
    "# raport_classify(data_all_train, data_label_train, np_red_data, np_red_col, data_all_test, data_label_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import plot_confusion_matrix\n",
    "\n",
    "\n",
    "classify_metrics = {\"accuracy\": accuracy_score,\n",
    "                    \"f1\": f1_score,\n",
    "                    \"precision\": precision_score,\n",
    "                    \"recall\": recall_score}\n",
    "\n",
    "                    ###################test\n",
    "classifier = classifiers['knn']\n",
    "classifier.fit(data_all_train, data_label_train)\n",
    "pred = classifier.predict(data_all_test)\n",
    "# data_all_train, data_label_train, np_red_data, np_red_col, data_all_test, data_label_test\n",
    "for metric in classify_metrics:\n",
    "    if (metric == 'accuracy'):\n",
    "        print(metric, \": \", classify_metrics[metric](data_label_test, pred))\n",
    "    else: \n",
    "        print(metric, \": \", classify_metrics[metric](data_label_test, pred, average=None))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def raport_classify(original_set, original_labels, reduced_set, reduced_labels, test_set, test_labels, c_type = 'all'):\n",
    "    \"\"\"\n",
    "    TODO special classifier\n",
    "    Function generating raport \n",
    "\n",
    "    :original_set: original training dataset\n",
    "    :original_labels: labels of classes in original trainign dataset\n",
    "    :reduced_set: reduced training dataset\n",
    "    :reduced_labels: labels of classes in reduced training dataset\n",
    "    :test_set: testing dataset\n",
    "    :test_labels: labels of classes in testing dataset, expected classification results\n",
    "    :c_type: name of classifier, optional attribute, if not given raport is generated for all classifiers\n",
    "                       Possible values: 'knn': KNeighborsClassifier(), \n",
    "                                        'svm': svm.SVC(), \n",
    "                                        'naive_bayers': GaussianNB(), \n",
    "                                        'decision_tree': DecisionTreeClassifier(),\n",
    "                                        'neutral_network': MLPClassifier().\n",
    "    \"\"\"\n",
    "    try:\n",
    "        if c_type == 'all':\n",
    "            for c_t in classifiers:\n",
    "                raport_classify(original_set, original_labels, reduced_set, reduced_labels, test_set, test_labels, c_t)\n",
    "        else:\n",
    "            #select classifier\n",
    "            classifier = classifiers[c_type]\n",
    "            #train with original dataset and time measure\n",
    "            start = time.clock()\n",
    "            classifier.fit(original_set, original_labels)\n",
    "            end = time.clock()\n",
    "            training_time = end - start\n",
    "\n",
    "            #make predictions and time measure\n",
    "            start = time.clock()\n",
    "            predict = classifier.predict(test_set)\n",
    "            end = time.clock()\n",
    "            prediction_time = end - start\n",
    "\n",
    "            #create confusion matrix\n",
    "            plot = plot_confusion_matrix(classifier, data_all_test, data_label_test, cmap=plt.cm.Blues)\n",
    "            title = \"Confusion matrix\\n original data - classifier: \" + str(c_type)\n",
    "            plot.ax_.set_title(title)\n",
    "            plot.figure_.savefig(\".\\plots\\\\\" + c_type + ' - original data')  \n",
    "\n",
    "            #print raport with metrics for original training data\n",
    "            print('=============')\n",
    "            print(\"Classifier:  \", c_type)\n",
    "            print('=============')\n",
    "            print(\"Raport for original dataset\")\n",
    "            print('Count of instances: ', len(original_labels))\n",
    "            for metric in classify_metrics:\n",
    "                if (metric == 'accuracy'):\n",
    "                    print(metric, \": \", classify_metrics[metric](test_labels, pred))\n",
    "                else: \n",
    "                    print(metric, \": \", classify_metrics[metric](test_labels, pred, average=None))\n",
    "            print('===')\n",
    "            print(\"Training time: \", training_time)\n",
    "            print(\"Predicting time: \", prediction_time)\n",
    "\n",
    "\n",
    "            #same for reduced training dataset\n",
    "            classifier = classifiers[c_type]\n",
    "            start = time.clock()\n",
    "            classifier.fit(reduced_set, reduced_labels)\n",
    "            end = time.clock()\n",
    "            training_time = end - start\n",
    "            start = time.clock()\n",
    "            predict = classifier.predict(test_set)\n",
    "            end = time.clock()\n",
    "            prediction_time = end - start\n",
    "\n",
    "            plot = plot_confusion_matrix(classifier, data_all_test, data_label_test, cmap=plt.cm.Blues)\n",
    "            title = \"Confusion matrix\\n reduced data - classifier: \" + str(c_type)\n",
    "            plot.ax_.set_title(title)\n",
    "            plot.figure_.savefig(\".\\plots\\\\\" + c_type + ' - reduced data')  \n",
    "\n",
    "            print(\"\\nRaport for reduced dataset\")\n",
    "            print('Count of instances: ', len(reduced_labels))\n",
    "            for metric in classify_metrics:\n",
    "                if (metric == 'accuracy'):\n",
    "                    print(metric, \": \", classify_metrics[metric](test_labels, pred))\n",
    "                else: \n",
    "                    print(metric, \": \", classify_metrics[metric](test_labels, pred, average=None))\n",
    "                \n",
    "            print('===')\n",
    "            print(\"Training time: \", training_time)\n",
    "            print(\"Predicting time: \", prediction_time, \"\\n\")\n",
    "    except KeyError:\n",
    "        print('Choose existing classifier!')\n",
    "    #except:\n",
    "     #   print('Check propriety of atributes!')\n",
    "\n",
    "raport_classify(data_all_train, data_label_train, np_red_data, np_red_col, data_all_test, data_label_test, 'knn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np_red_col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_label_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.delete(data_label_train, [0,1], axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn = KNeighborsClassifier(n_neighbors= 4).fit(data_all_train, data_label_train)\n",
    "graph = knn.kneighbors_graph().toarray()\n",
    "indx = knn.kneighbors([data_all_train[0]], return_distance = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph[4013,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_ins = len(data_all_train)\n",
    "NN = np.empty((n_ins), dtype=object)\n",
    "NA = np.empty((n_ins), dtype=object)\n",
    "NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NN[0] = np.where(graph[0] == 1)[0] #indexy sąsiadów\n",
    "NA[0] = np.where(graph[:,0] == 1)[0] #indexy associates\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NN[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn = KNeighborsClassifier(n_neighbors= 4).fit(data_all_train, data_label_train)\n",
    "graph = knn.kneighbors_graph().toarray()\n",
    "\n",
    "n_ins = len(data_all_train)\n",
    "NN = np.empty((n_ins), dtype=object)\n",
    "AN = np.empty((n_ins), dtype=object)\n",
    "\n",
    "for i in range(n_ins):\n",
    "    NN[i] = np.where(graph[i] == 1)[0] #indexes of neighbours\n",
    "    AN[i] = np.where(graph[:,i] == 1)[0] #indexes of associates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from scipy.spatial import distance\n",
    "\n",
    "arr = distance.cdist(data_all_train, data_all_train)\n",
    "\n",
    "tmp = np.arange(n_ins) #array of original indexes\n",
    "\n",
    "for i in range(n_ins):\n",
    "    #arr1 = np.vstack((arr[i], np.array(tmp)))\n",
    "    #array o\n",
    "    x = np.argsort(arr[i])\n",
    "    neigh = []\n",
    "    enemy = []\n",
    "    #arr1.sort(axis = 1)\n",
    "    for j in x:\n",
    "        if data_label_train[i] == data_label_train[j]:\n",
    "            neigh.append(j)\n",
    "        else:\n",
    "            enemy.append(j)\n",
    "\n",
    "\n",
    "x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lab = list(class_dict.keys())\n",
    "n_cl = dict.fromkeys(lab, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lab = list(class_dict.keys())\n",
    "n_cl = dict.fromkeys(lab, 0)\n",
    "n_cl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn4 = KNeighborsClassifier()\n",
    "knn4.predict([[93,4]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def group_neigh_enemies(labels, index, sort):\n",
    "        \"\"\"\n",
    "        Function grouping indexes of points with same label and enemies - points with different label.\n",
    "        Atributes:\n",
    "        :labels: - array of label for each point in dataset\n",
    "        :index: - index of point\n",
    "        :dist_arr: - 1d array of indexes sorted by distance beetween :index:\n",
    "        Return arrays:\n",
    "        :neigh: - array of points with same label as point with :index:\n",
    "        :enemy: - array of points with diferent label than point with :index:\n",
    "        \"\"\"\n",
    "        #init empty arrays\n",
    "        neigh = []\n",
    "        enemy = []\n",
    "        for i in sort:\n",
    "                if labels[index] == labels[i]:\n",
    "                    neigh.append(i)\n",
    "                else:\n",
    "                    enemy.append(i)\n",
    "\n",
    "        return neigh, enemy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "red_data = []\n",
    "red_lab = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "        dist_arr = distance.cdist(data_all_train, data_all_train)\n",
    "        n_ins = len(data_all_train)\n",
    "        tmp = np.arange(n_ins) #array of original indexes\n",
    "        nearest_enemy = []\n",
    "        nearest_enemy_dist = []\n",
    "    \n",
    "\n",
    "        #create array with indexes of nearest enemy\n",
    "        for i in range(n_ins):\n",
    "            #sort by distance\n",
    "            sort = np.argsort(dist_arr[i])\n",
    "            #create sorted array with indexes of neighbours with same label and enemies - with different label\n",
    "            neigh, enemy = group_neigh_enemies(data_label_train, i, sort)\n",
    "            #add index of nearest enemy to aray \n",
    "            nearest_enemy.append(enemy[0])\n",
    "            nearest_enemy_dist.append(dist_arr[i][enemy[0]])\n",
    "\n",
    "        #indexes sorted by nearest enemy distance\n",
    "        sort = np.argsort(nearest_enemy_dist)\n",
    "        s = sort\n",
    "        added = []\n",
    "        for i in sort:\n",
    "            add = False\n",
    "            for j in sort:\n",
    "                if j in s and dist_arr[i][j] < nearest_enemy_dist[j]:\n",
    "                    s = np.delete(s, np.where(s==j))\n",
    "                    add = True\n",
    "            \n",
    "            if add and i not in added:\n",
    "                red_data.append(data_all_train[i])\n",
    "                red_lab.append(data_label_train[i])\n",
    "                added.append(i)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.array([])\n",
    "for i in range(n_ins):\n",
    "    a = np.append(a, np.array(list))\n",
    "\n",
    "a[56] = [4.565,546.546,7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_dict = {'i1': 0, 'i2': 1, 'i3': 3}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lab = list(class_dict.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "red_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dist_arr[8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assot = []\n",
    "\n",
    "for i in range(n_ins):\n",
    "    # np.append(assot, 6)\n",
    "    assot.append(np.argsort(dist_arr[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_csv('D:\\Studia\\inz\\datasets\\\\abalone.data.txt', sep = ',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "object in dataset.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "for col in data_all.columns:\n",
    "    print(is_numeric_dtype(data_all[col]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.dtypes.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_all.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_all = dataset.drop(columns='M')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('File {} not found'.format(n_ins))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'knn' not in classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(name = None, filepath = None, class_col = 'class', sep = ','):\n",
    "        \"\"\"\n",
    "        Function loading dataset to pandas dataframe\n",
    "        \"\"\"\n",
    "\n",
    "        if name and filepath == None:\n",
    "            raise Exception('Cant load data without name of dataset or filepath of dataset file!')\n",
    "\n",
    "        #open file\n",
    "        if filepath:\n",
    "            try: \n",
    "                dataset = pd.read_csv(filepath, sep = sep)\n",
    "                load_csv()\n",
    "                prepare_dataset()\n",
    "            except FileNotFoundError:\n",
    "                raise Exception(\"File {} not found. Please select the existing csv file!\".format(filepath))\n",
    "            except OSError:\n",
    "                raise Exception('Can not use file in path {}. Please select the apriopriate filepath!'.format(filepath))\n",
    "            except ValueError:\n",
    "                raise Exception('Can not use file in path {}. Please select filepath with apriopriate extension!'.format(filepath))\n",
    "            except:\n",
    "                raise Exception('Please select csv filepath with apriopriate extension.')\n",
    "\n",
    "        elif name:\n",
    "            if self.dataset_name not in dataset_path:\n",
    "                raise Exception('Dataset {} not found. Please select apriopriate dataset name!'.format(name))\n",
    "            else:\n",
    "                dataset = pd.read_csv(dataset_path[self.dataset_name])\n",
    "                load_named()\n",
    "                prepare_dataset()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NotLoadedException(Exception):\n",
    "    def __init__(self, message):\n",
    "        self.message = message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas.api.types import is_numeric_dtype\n",
    "def from_csv(filepath, class_col, sep = ','):\n",
    "    \n",
    "    #check loaded\n",
    "    try:\n",
    "        dataset == None\n",
    "    except NameError:\n",
    "        raise NotLoadedException('Dataset must be loaded before preparation')\n",
    "\n",
    "    #constains more than one column\n",
    "    if len(dataset.columns) < 2:\n",
    "        raise Exception('Dataset must have minimum 2 columns! Please check if you select apriopriate filepath or separator.')\n",
    "\n",
    "    #contains missing values\n",
    "    if dataset.isnull().values.any() == True:\n",
    "        raise Exception('Dataset contains Nan values. Please fill missing values before use class.')\n",
    "\n",
    "    #constains class column\n",
    "    if class_col in dataset.columns:\n",
    "        data_label = dataset[class_col]\n",
    "        data_all = dataset.drop(columns=class_col)\n",
    "        features = data_all.columns.values.tolist()\n",
    "    else:\n",
    "        raise Exception('Please select existing column name as class column.')\n",
    "\n",
    "    #constains only numeric values\n",
    "    for col in data_all.columns:\n",
    "        if not is_numeric_dtype(data_all[col]):\n",
    "            raise Exception('Dataset constains non numeric values.')\n",
    "\n",
    "    #reszta tak jak jest od normalizacji\n",
    "    \n",
    "\n",
    "\n",
    "     \n",
    "\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Data:\n",
    "\n",
    "    def __init__(self, name = None, ):\n",
    "        \n",
    "        #if one arg:\n",
    "        if len(args) == 1:\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(None) !=str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################\n",
    "######################\n",
    "#TODO all function \n",
    "#find variebles usen in defs\n",
    "#class_dict, \n",
    "def main_clustering_reduction(data: DataPreparation, n_clusters = 20):\n",
    "    \"\"\"\n",
    "    The main function in module, runs all required functions for data and returns reduced data\n",
    "\n",
    "    TODO data must be a class consisting of data_all, class_map, n_classes etc.\n",
    "    \"\"\"\n",
    "    #create clusters\n",
    "    clusters = create_clusters(data = data_all_train, number_of_clusters = n_clusters)\n",
    "    #group clusters \n",
    "    clusters_with_id = group_id_by_cluster(clusters = clusters)\n",
    "    #run algorithm\n",
    "\n",
    "    np_red_data, np_red_col = clustering_reduction(clusters_with_id, data_all_train)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rel = \"D:\\Studia\\inz\\Repos\\DataReduction\\Module\\InstanceReduction\\Reduction\\__init__.py\"\n",
    "rel = \"D:\\Studia\\inz\\Repos\\DataReduction\\README.md\"\n",
    "absol = \"D:\\Studia\\inz\\Repos\\DataReduction\\Module\\InstanceReduction\\datasets_csv\\glass.csv\"\n",
    "shit = \"xx\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "if not os.path.isabs(rel):\n",
    "    rel = os.path.join(os.getcwd(),rel)\n",
    "\n",
    "dataset = pd.read_csv(rel, sep = ' ')\n",
    "dataset.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset['f']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.iloc[:, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = pd.read_csv(\"D:\\Studia\\inz\\Repos\\DataReduction\\Module\\InstanceReduction\\datasets_csv\\iris.csv\")\n",
    "lab = d.iloc[:,4].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {'iris': [150, 3], \n",
    "    'pendigits': [10992, 10], \n",
    "    'letter': [20000, 26]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params=[150, 10992, 20000]\n",
    "n =['iris', 'pendigits', 'letter']\n",
    "tuple(params,n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def test_len_data_all(dataset_names, len_data_all):\n",
    "    d = DataPreparation(dataset_names)\n",
    "    assert len(d.data_all) == len_data_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col = 4\n",
    "if col not in d.columns:\n",
    "    if col > len(d.columns):\n",
    "        raise IndexError('Index of class column is out of range')\n",
    "    elif type(col) == int:\n",
    "        print(d.iloc[:,col])\n",
    "d = d.drop(d.columns[col], axis = 1)\n",
    "\n",
    "d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class x:\n",
    "    pass\n",
    "\n",
    "xi = -1.45 #x()\n",
    "if not isinstance(xi, x):\n",
    "    raise TypeError('Atribute \\'data\\' must be DataPreparation instance')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array(DataPreparation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "arr = np.array([4,5,66])\n",
    "type(arr) == np.ndarray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lis[-4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lis = [5,6,7]\n",
    "\n",
    "for l in [arr, lis]:\n",
    "    print(type(l) == list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i =0\n",
    "i in range(0,5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "arr = np.arange(5)\n",
    "arr.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python37764bitb4812af2bc4947f3b7e7c9dd0123bdb7",
   "display_name": "Python 3.7.7 64-bit"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}